{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install -q -r requirements.txt",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Literal\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import requests"
   ],
   "id": "5bde2e744441d793",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataclasses to make things easier\n",
    "@dataclass\n",
    "class Passage:\n",
    "    pid: str\n",
    "    text: str\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class Example:\n",
    "    qid: str\n",
    "    question: str\n",
    "    answer: Optional[str]\n",
    "    context: List[Passage]\n",
    "    time: Optional[str]\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    pid: str\n",
    "    score: float\n",
    "    text: str\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "@dataclass\n",
    "class QATrace:\n",
    "    qid: str\n",
    "    question: str\n",
    "    hop_queries: List[str]\n",
    "    retrieved: List[RetrievalResult]\n",
    "    evidence: List[RetrievalResult]\n",
    "    predicted_answer: str"
   ],
   "id": "bea999cf2f149dba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_hotpotqa(hf_id: str = \"hotpotqa/hotpot_qa\", config: str = \"fullwiki\", train_split: str = \"train\", test_split: str = \"validation\", max_context_passages: Optional[int] = None) -> Dict[str, List[Example]]:\n",
    "    ds_train = load_dataset(hf_id, config, split=train_split)\n",
    "    ds_test = load_dataset(hf_id, config, split=test_split)\n",
    "\n",
    "    def row_to_example(row) -> Example:\n",
    "        qid = str(row[\"id\"])\n",
    "        question = row[\"question\"]\n",
    "        answer = row.get(\"answer\", None)\n",
    "\n",
    "        titles, sents_lists = row[\"context\"]\n",
    "        passages: List[Passage] = []\n",
    "        for i, (title, sents) in enumerate(zip(titles, sents_lists)):\n",
    "            text = \" \".join(sents) if isinstance(sents, list) else str(sents)\n",
    "            passages.append(Passage(\n",
    "                pid=f\"{qid}::ctx::{i}\",\n",
    "                text=text,\n",
    "                meta={\"title\": title}\n",
    "            ))\n",
    "            if max_context_passages and len(passages) >= max_context_passages:\n",
    "                break\n",
    "\n",
    "        meta = {\n",
    "            \"type\": row.get(\"type\"),\n",
    "            \"level\": row.get(\"level\"),\n",
    "            \"supporting_facts\": row.get(\"supporting_facts\"),\n",
    "        }\n",
    "\n",
    "        return Example(\n",
    "            qid=qid,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            context=passages,\n",
    "            time=None,\n",
    "            meta=meta,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"train\": [row_to_example(r) for r in ds_train],\n",
    "        \"test\":  [row_to_example(r) for r in ds_test],\n",
    "    }"
   ],
   "id": "6692f959b8ff8364",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_chronicling_america_qa(hf_id: str = \"Bhawna/ChroniclingAmericaQA\", train_split: str = \"train\", test_split: str = \"test\", use_clean_context: bool = True) -> Dict[str, List[Example]]:\n",
    "    ds_train = load_dataset(hf_id, split=train_split)\n",
    "    ds_test = load_dataset(hf_id, split=test_split)\n",
    "\n",
    "    def row_to_example(row) -> Example:\n",
    "        qid = row[\"query_id\"]\n",
    "        question = row[\"question\"]\n",
    "\n",
    "        answer = row.get(\"answer\") or row.get(\"org_answer\")\n",
    "\n",
    "        text = row.get(\"context\") if use_clean_context else row.get(\"raw ocr\")\n",
    "        if text is None:\n",
    "            text = \"\"\n",
    "\n",
    "        pid = str(row.get(\"para id\", qid))\n",
    "        pub_date = row.get(\"publication date\")\n",
    "\n",
    "        passage = Passage(\n",
    "            pid=pid,\n",
    "            text=text,\n",
    "            meta={\"url\": row.get(\"url\"), \"raw_ocr\": row.get(\"raw ocr\") if use_clean_context else None}\n",
    "        )\n",
    "\n",
    "        meta = {\n",
    "            \"trans_que\": row.get(\"trans que\"),\n",
    "            \"trans_ans\": row.get(\"trans ans\"),\n",
    "            \"url\": row.get(\"url\"),\n",
    "            \"para_id\": row.get(\"para id\"),\n",
    "        }\n",
    "\n",
    "        return Example(\n",
    "            qid=qid,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            context=[passage],\n",
    "            time=str(pub_date) if pub_date is not None else None,\n",
    "            meta=meta,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"train\": [row_to_example(r) for r in ds_train],\n",
    "        \"test\":  [row_to_example(r) for r in ds_test],\n",
    "    }\n"
   ],
   "id": "391788eb3e159804",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_timeqa(hf_id: str = \"hugosousa/TimeQA\", train_split: str = \"train\", test_split: str = \"test\") -> Dict[str, List[Example]]:\n",
    "    ds_train = load_dataset(hf_id, split=train_split)\n",
    "    ds_test = load_dataset(hf_id, split=test_split)\n",
    "\n",
    "    def normalize_answer(targets) -> Optional[str]:\n",
    "        if targets is None:\n",
    "            return None\n",
    "        if isinstance(targets, str):\n",
    "            return targets\n",
    "        if isinstance(targets, list):\n",
    "            return targets[0] if len(targets) > 0 else None\n",
    "        return str(targets)\n",
    "\n",
    "    def row_to_example(row) -> Example:\n",
    "        qid = str(row[\"idx\"])\n",
    "        question = row[\"question\"]\n",
    "        answer = normalize_answer(row.get(\"targets\"))\n",
    "\n",
    "        ctx = row.get(\"context\")\n",
    "        passages: List[Passage] = []\n",
    "        if isinstance(ctx, list):\n",
    "            for i, t in enumerate(ctx):\n",
    "                passages.append(Passage(pid=f\"{qid}::p{i}\", text=str(t), meta={}))\n",
    "        else:\n",
    "            passages.append(Passage(pid=f\"{qid}::p0\", text=str(ctx) if ctx is not None else \"\", meta={}))\n",
    "\n",
    "        meta = {\"level\": row.get(\"level\")}\n",
    "        return Example(\n",
    "            qid=qid,\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "            context=passages,\n",
    "            time=None,\n",
    "            meta=meta,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"train\": [row_to_example(r) for r in ds_train],\n",
    "        \"test\":  [row_to_example(r) for r in ds_test],\n",
    "    }"
   ],
   "id": "a7330e76a91e578f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def examples_to_passages(examples: List[Example]) -> List[Dict[str, Any]]:\n",
    "    # Use all passages from the split you index (train recommended for in-domain)\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for ex in examples:\n",
    "        for p in ex.context:\n",
    "            if p.pid in seen:\n",
    "                continue\n",
    "            seen.add(p.pid)\n",
    "            out.append({\"pid\": p.pid, \"text\": p.text, \"meta\": p.meta})\n",
    "    return out\n",
    "\n",
    "def build_passage_store(passages: List[Dict[str, Any]]):\n",
    "    pids = [p[\"pid\"] for p in passages]\n",
    "    texts = [p[\"text\"] for p in passages]\n",
    "    metas = [p.get(\"meta\", {}) for p in passages]\n",
    "    pid_to_text = {pid: txt for pid, txt in zip(pids, texts)}\n",
    "    pid_to_meta = {pid: m for pid, m in zip(pids, metas)}\n",
    "    return pids, texts, metas, pid_to_text, pid_to_meta"
   ],
   "id": "ffd1c757f2f382be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# BM25 retriever\n",
    "class BM25Retriever:\n",
    "    def __init__(self, pids, texts):\n",
    "        self.pids = pids\n",
    "        self.texts = texts\n",
    "        tokenized = [t.lower().split() for t in texts]\n",
    "        self.bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        q_tok = query.lower().split()\n",
    "        scores = self.bm25.get_scores(q_tok)\n",
    "        top_idx = np.argsort(scores)[::-1][:k]\n",
    "        return [(self.pids[i], float(scores[i]), i) for i in top_idx]"
   ],
   "id": "8b14a2aa456023e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DenseRetriever:\n",
    "    def __init__(self, pids, texts, model_name=os.getenv(\"DENSE_MODEL\"), batch_size=64):\n",
    "        self.pids = pids\n",
    "        self.texts = texts\n",
    "        self.model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "\n",
    "        emb = self.model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=True\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "        self.dim = emb.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(self.dim)\n",
    "        self.index.add(emb)\n",
    "\n",
    "    def retrieve(self, query, k=10):\n",
    "        q = self.model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
    "        scores, idxs = self.index.search(q, k)\n",
    "        idxs = idxs[0]\n",
    "        scores = scores[0]\n",
    "        return [(self.pids[i], float(scores[j]), int(i)) for j, i in enumerate(idxs)]"
   ],
   "id": "63ccbe976b6f1fd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HybridRetriever:\n",
    "    def __init__(self, bm25_ret: BM25Retriever, dense_ret: DenseRetriever, alpha=0.5):\n",
    "        self.bm25_ret = bm25_ret\n",
    "        self.dense_ret = dense_ret\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(scores):\n",
    "        s = np.array(scores, dtype=\"float32\")\n",
    "        if len(s) == 0:\n",
    "            return s\n",
    "\n",
    "        mean = s.mean()\n",
    "        std = s.std()\n",
    "        if std < 1e-8:\n",
    "            return np.zeros_like(s)\n",
    "\n",
    "        z = (s - mean) / std\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def retrieve(self, query, k=10, candidate_k=50):\n",
    "        bm = self.bm25_ret.retrieve(query, k=candidate_k)\n",
    "        de = self.dense_ret.retrieve(query, k=candidate_k)\n",
    "\n",
    "        bm_map = {pid: score for pid, score, _ in bm}\n",
    "        de_map = {pid: score for pid, score, _ in de}\n",
    "\n",
    "        # union of candidates\n",
    "        all_pids = list(set(bm_map.keys()) | set(de_map.keys()))\n",
    "        bm_scores = [bm_map.get(pid, 0.0) for pid in all_pids]\n",
    "        de_scores = [de_map.get(pid, 0.0) for pid in all_pids]\n",
    "\n",
    "        bm_norm = self._normalize(bm_scores)\n",
    "        de_norm = self._normalize(de_scores)\n",
    "\n",
    "        hybrid = self.alpha * bm_norm + (1.0 - self.alpha) * de_norm\n",
    "        order = np.argsort(hybrid)[::-1][:k]\n",
    "\n",
    "        return [(all_pids[i], float(hybrid[i]), None) for i in order]"
   ],
   "id": "f177a57ba24f1f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QueryAgent:\n",
    "    def make_query(self, question: str, hop_context: Optional[str] = None) -> str:\n",
    "        return question"
   ],
   "id": "6b7c850475d2474f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RetrieverAdapter:\n",
    "    def __init__(self, retriever, pid_to_text, pid_to_meta):\n",
    "        self.retriever = retriever\n",
    "        self.pid_to_text = pid_to_text\n",
    "        self.pid_to_meta = pid_to_meta\n",
    "\n",
    "    def retrieve(self, query: str, k: int) -> List[RetrievalResult]:\n",
    "        raw = self.retriever.retrieve(query, k=k)\n",
    "        out = []\n",
    "        for pid, score, _ in raw:\n",
    "            out.append(RetrievalResult(\n",
    "                pid=pid,\n",
    "                score=score,\n",
    "                text=self.pid_to_text.get(pid, \"\"),\n",
    "                meta=self.pid_to_meta.get(pid, {}),\n",
    "            ))\n",
    "        return out"
   ],
   "id": "feecbdaea59ec187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EvidenceSelector:\n",
    "    def __init__(self, top_n: int = 3):\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def select(self, retrieved: List[RetrievalResult]) -> List[RetrievalResult]:\n",
    "        return retrieved[: self.top_n]"
   ],
   "id": "ee104c5556eb624a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ollama\n",
    "\n",
    "def _build_context(evidence, max_chars_per_passage: int = 2000) -> str:\n",
    "    blocks: List[str] = []\n",
    "    for i, r in enumerate(evidence, start=1):\n",
    "        txt = (r.text or \"\")[:max_chars_per_passage]\n",
    "        blocks.append(f\"[Passage {i}]\\n{txt}\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "def _build_prompt(question: str, context: str) -> List[dict[str, str]]:\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a question answering assistant. Use only the provided passages as evidence. If the answer is not in the passages, say 'Insufficient information.'\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\\n\\nPassages:\\n{context}\\n\\nAnswer (concise):\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        backend: Literal[\"ollama\", \"openai\"] = \"ollama\",\n",
    "        ollama_model: str = os.getenv(\"OLLAMA_MODEL\", \"gemma3:4b\"),\n",
    "        ollama_url: str = \"http://localhost:11434\",\n",
    "        openai_model: str = \"gpt-4o-mini\",\n",
    "        openai_api_key: Optional[str] = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_passages: int = 5,\n",
    "        max_chars_per_passage: int = 2000,\n",
    "    ):\n",
    "        self.backend = backend\n",
    "        self.ollama_model = ollama_model\n",
    "        self.ollama_url = ollama_url.rstrip(\"/\")\n",
    "        self.openai_model = openai_model\n",
    "        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.temperature = temperature\n",
    "        self.max_passages = max_passages\n",
    "        self.max_chars_per_passage = max_chars_per_passage\n",
    "\n",
    "    def answer(self, question: str, evidence) -> str:\n",
    "        evidence = evidence[: self.max_passages]\n",
    "        context = _build_context(evidence, max_chars_per_passage=self.max_chars_per_passage)\n",
    "        message = _build_prompt(question, context)\n",
    "\n",
    "        if self.backend == \"ollama\":\n",
    "            return self._answer_ollama(message)\n",
    "        elif self.backend == \"openai\":\n",
    "            return self._answer_openai(message)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backend: {self.backend}\")\n",
    "\n",
    "    def _answer_ollama(self, message: List[dict[str, str]]) -> str:\n",
    "        # payload = {\n",
    "        #     \"model\": self.ollama_model,\n",
    "        #     \"messages\": messages,\n",
    "        #     \"stream\": False,\n",
    "        #     \"options\": {\"temperature\": self.temperature},\n",
    "        # }\n",
    "        resp = ollama.chat(\n",
    "            model = self.ollama_model,\n",
    "            messages = message,\n",
    "            options = {\"temperature\": self.temperature},\n",
    "        )\n",
    "        #resp = requests.post(f\"{self.ollama_url}/api/chat\", json=payload)\n",
    "        #resp.raise_for_status()\n",
    "        #data = resp.json()\n",
    "        return resp.message.content.strip()\n",
    "\n",
    "    def _answer_openai(self, messages: List[dict[str, str]]) -> str:\n",
    "        if not self.openai_api_key:\n",
    "            raise RuntimeError(\"OPENAI_API_KEY not set. Set env var or pass openai_api_key=...\")\n",
    "\n",
    "        client = OpenAI(api_key=self.openai_api_key)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=self.openai_model,\n",
    "            messages=messages,\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n"
   ],
   "id": "ce2bbb10d904b392",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class QASystem:\n",
    "    def __init__(self, query_agent, retriever_adapter, evidence_selector, answer_generator):\n",
    "        self.query_agent = query_agent\n",
    "        self.retriever = retriever_adapter\n",
    "        self.evidence_selector = evidence_selector\n",
    "        self.answer_generator = answer_generator\n",
    "\n",
    "    def run_one(self, qid: str, question: str, top_k: int = 10) -> QATrace:\n",
    "        q = self.query_agent.make_query(question)\n",
    "        retrieved = self.retriever.retrieve(q, k=top_k)\n",
    "        evidence = self.evidence_selector.select(retrieved)\n",
    "        pred = self.answer_generator.answer(question, evidence)\n",
    "        return QATrace(qid=qid, question=question, hop_queries=[q], retrieved=retrieved, evidence=evidence, predicted_answer=pred)"
   ],
   "id": "d6a66395555c2863",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def exact_match(pred: str, gold: str) -> float:\n",
    "    return 1.0 if _normalize_text(pred) == _normalize_text(gold) else 0.0\n",
    "\n",
    "def f1_score(pred: str, gold: str) -> float:\n",
    "    pred_toks = _normalize_text(pred).split()\n",
    "    gold_toks = _normalize_text(gold).split()\n",
    "    if not pred_toks and not gold_toks:\n",
    "        return 1.0\n",
    "    if not pred_toks or not gold_toks:\n",
    "        return 0.0\n",
    "    common = Counter(pred_toks) & Counter(gold_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_toks)\n",
    "    recall = num_same / len(gold_toks)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def extract_years(text: str) -> List[int]:\n",
    "    return [int(y) for y in re.findall(r\"\\b(19\\d{2}|20\\d{2})\\b\", text)]\n",
    "\n",
    "def parse_year_from_meta(meta: Dict[str, Any]) -> Optional[int]:\n",
    "    if \"year\" in meta:\n",
    "        try:\n",
    "            return int(meta[\"year\"])\n",
    "        except:\n",
    "            pass\n",
    "    for k in [\"publication_date\", \"publication date\", \"date\", \"time\"]:\n",
    "        if k in meta and meta[k]:\n",
    "            ys = extract_years(str(meta[k]))\n",
    "            if ys:\n",
    "                return ys[0]\n",
    "    return None\n",
    "\n",
    "def temporal_score(question: str, evidence: List[RetrievalResult]) -> Optional[float]:\n",
    "    q_years = set(extract_years(question))\n",
    "    if not q_years:\n",
    "        return None\n",
    "    ev_years = set()\n",
    "    for r in evidence:\n",
    "        y = parse_year_from_meta(r.meta)\n",
    "        if y is not None:\n",
    "            ev_years.add(y)\n",
    "    if not ev_years:\n",
    "        return 0.0\n",
    "    return 1.0 if len(q_years & ev_years) > 0 else 0.0"
   ],
   "id": "e8d01f1c2cd6e139",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_eval(qa_system: QASystem, examples: List[Example], top_k: int = 10, max_examples: Optional[int] = None):\n",
    "    em_sum = f1_sum = 0.0\n",
    "    n_scored = 0\n",
    "    t_sum = 0.0\n",
    "    t_n = 0\n",
    "    traces = []\n",
    "\n",
    "    use = examples[:max_examples] if max_examples else examples\n",
    "    for ex in tqdm(use):\n",
    "        trace = qa_system.run_one(ex.qid, ex.question, top_k=top_k)\n",
    "\n",
    "        if ex.answer is not None:\n",
    "            em_sum += exact_match(trace.predicted_answer, ex.answer)\n",
    "            f1_sum += f1_score(trace.predicted_answer, ex.answer)\n",
    "            n_scored += 1\n",
    "\n",
    "        ts = temporal_score(ex.question, trace.evidence)\n",
    "        if ts is not None:\n",
    "            t_sum += ts\n",
    "            t_n += 1\n",
    "\n",
    "        traces.append({\n",
    "            \"qid\": ex.qid,\n",
    "            \"question\": ex.question,\n",
    "            \"gold\": ex.answer,\n",
    "            \"pred\": trace.predicted_answer,\n",
    "            \"query\": trace.hop_queries[0],\n",
    "            \"evidence_pids\": [r.pid for r in trace.evidence],\n",
    "            \"evidence_years\": [parse_year_from_meta(r.meta) for r in trace.evidence],\n",
    "        })\n",
    "\n",
    "    metrics = {\n",
    "        \"EM\": em_sum / max(n_scored, 1),\n",
    "        \"F1\": f1_sum / max(n_scored, 1),\n",
    "        \"Temporal@evidence\": (t_sum / t_n) if t_n > 0 else None,\n",
    "        \"n_scored\": n_scored,\n",
    "        \"n_temporal_scored\": t_n,\n",
    "        \"top_k\": top_k,\n",
    "    }\n",
    "    return metrics, traces"
   ],
   "id": "4ab46415f8439879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_dataset(train_examples: List[Example], test_examples: List[Example], retriever_kind=\"hybrid\"):\n",
    "    passages = examples_to_passages(train_examples)\n",
    "    pids, texts, metas, pid_to_text, pid_to_meta = build_passage_store(passages)\n",
    "\n",
    "    bm25 = BM25Retriever(pids, texts)\n",
    "    dense = DenseRetriever(pids, texts, model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    hybrid = HybridRetriever(bm25, dense, alpha=0.5)\n",
    "\n",
    "    base = {\"bm25\": bm25, \"dense\": dense, \"hybrid\": hybrid}[retriever_kind]\n",
    "    retr_adapter = RetrieverAdapter(base, pid_to_text, pid_to_meta)\n",
    "\n",
    "    answer_ollama = AnswerGenerator(backend=\"ollama\")\n",
    "    qa_system = QASystem(QueryAgent(), retr_adapter, EvidenceSelector(top_n=20), answer_ollama)\n",
    "\n",
    "    metrics, traces = run_eval(qa_system, test_examples, top_k=20, max_examples=1000)\n",
    "    print(retriever_kind, metrics)\n",
    "    return metrics, traces"
   ],
   "id": "be7d6721830ee50f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hot = load_hotpotqa()\n",
    "metrics, traces = run_dataset(hot[\"train\"], hot[\"test\"], retriever_kind=\"bm25\")"
   ],
   "id": "9f8f753419b2b99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "724d369bb36e58c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bd599afe8f599398",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
